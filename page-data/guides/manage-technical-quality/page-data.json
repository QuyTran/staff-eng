{"componentChunkName":"component---src-templates-story-template-js","path":"/guides/manage-technical-quality","result":{"data":{"markdownRemark":{"html":"<p>e reasons why best practices have such a bad reputation. In theory, organizations would benefit from adopting best practices before fixing quality hot spots, but I recommend practices after hot spotting. Adopting best practices requires a level of organizational and leadership maturity that takes some time to develop.</p>\n<p>When you're rolling out a new practice, remember that a <a href=\"https://lethain.com/good-process-is-evolved/\">good process is evolved</a> rather than mandated. Study how other companies adopt similar practices, document your intended approach, experiment with the practice with a few engaged teams, sand down the rough edges, improve the documentation based on the challenges, and only then roll it out further. A rushed process is a failed process.</p>\n<p>Equally important is the idea of limiting concurrent process rollouts. If you try to get teams to adopt multiple new practices simultaneously, you're fighting for their attention with yourself. It also makes it harder to attribute impact later if you're considering reverting or modifying one of the new practices. It's a bit draconian, but I've come to believe that you ought to limit yourself to a single best practice rollout at any given time. Channel all your energy towards making one practice a success rather than splitting resources across a handful.</p>\n<p>Adopting a single new practice at a time also forces you to think carefully about which to prioritize. Selecting your next process sounds easy, but it's often unclear which best practices are genuinely best practice and which are just familiar or famous. Genuine best practice has to be supported by research, and the best source of research on this topic is <a href=\"https://www.amazon.com/dp/B07B9F83WM/\">Accelerate</a>.</p>\n<p>While all of Accelerate's recommendations are data-driven and quite good, the handful that I've found most helpful to adopt early are version control, trunk-based development, CI/CD, and production observability (including developers on-call for the systems they write), and working in small, atomic changes. There are many other practices I'd love to advocate for, who hasn't spent a career era advocating for <a href=\"https://increment.com/documentation/why-investing-in-internal-docs-is-worth-it/\">better internal documentation</a>, but I don't trust my intuition like I once did.</p>\n<p>The transition from fixing hot spots to adopting best practices comes when you're overwhelmed by too many hot spots to cool. The next transition, from best practices to leverage points, comes when you find yourself wanting to adopt a new best practice before your in-progress best practice is working. Rather than <a href=\"https://lethain.com/limiting-wip/\">increasing your best practice adoption-in-progress limit</a>, move on to the next tool.</p>\n<h2>Leverage points</h2>\n<p>In the Hotspotting section, we talked about using the performance engineer's mindset to identify the right problems to fix. Optimization works well for the issues you already have, but it's intentionally inapplicable to the future: the worst sin of performance engineering is applying effort to unproven problems.</p>\n<p>However, as you look at how software changes over time, there are a small handful of places where extra investment preserves quality over time, both by preventing gross quality failures and reducing the cost of future quality investments.</p>\n<p>I call those quality leverage points, and the three most impactful points are interfaces, stateful systems, and data models.</p>\n<p><em>Interfaces</em> are contracts between systems. Effective interfaces decouple clients from the encapsulated implementation. Durable interfaces expose all the underlying essential complexity and none of the underlying accidental complexity. Delightful interfaces are <a href=\"https://increment.com/apis/api-design-for-eager-discering-developers/\">Eagerly discerning, discerningly eager</a>.</p>\n<p>State is the hardest part of any system to change, and that resistance to change makes <em>stateful systems</em> another critical leverage point. State's inertia comes from its scale. From its tendency to accrete complexity on behalf of availability, reliability, and compliance properties. From having the sort of correctness described with probabilities rather than theorems. (Or in the case of distributed state, both probabilities and theorems.)</p>\n<p><em>Data models</em> are the intersection of the interfaces and state, constraining your stateful system's capabilities down to what your application considers legal. A good data model is rigid: it only exposes what it genuinely supports and prevents invalid states' expression. A good data model is tolerant of evolution over time. Effective data models are not even slightly clever.</p>\n<p>As you identify these leverage points in your work, take the extra time to approach them deliberately. If it's an interface, integrate half a dozen clients against the mocked implementation. If it's a data model, represent half a dozen real scenarios. If it's stateful, exercise the failure modes, check the consistency behaviors, and establish performance benchmarks resembling your production scenario.</p>\n<p>Take everything you've learned, and pull it into a technical specification document that you socialize across your team. Gather industry feedback from peers. Even after you begin implementation, listen to reality's voice and remain open to changes.</p>\n<p>One of the hidden powers of investing in leverage points is that you don't need total organizational alignment to do it. To write a technical vision or roll out a best practice, you need that sort of buy-in, which is why I recommend starting with leverage points. However, if you've exhausted the accessible impact from leverage points, it may be time to move on to driving broader organizational alignment.</p>\n<h2>Technical vectors</h2>\n<p>Effective organizations marshall the majority of their efforts towards a shared vision. If you plot every project, every technical decision really, as a vector on a grid, the more those vectors point in the same direction, the more you’ll accomplish over time. Conversely, some of the most impressive engineers I’ve worked with created vectors with an extraordinary magnitude but a misaligned direction, and ultimately harmed their organization as they attempted to lead it.</p>\n<p>One sure-fire solution to align technical direction is to route all related decisions to the same person with <a href=\"https://staffeng.com/guides/staff-archetypes\">Architect</a> somewhere in their title. This works well, but is challenging to scale, and the quality of an architect’s decisions degrade the further they get from doing real work on real code in the real process. On the other extreme, you can allow every team to make independent decisions. But an organization that allows any tool is an organization with uniformly unsupported tooling.</p>\n<p>Your fundamental tools for aligning technical vectors are:</p>\n<ul>\n<li><strong>Give direct feedback.</strong> When folks run into misalignment, the first answer is often process change, but instead start with simply giving direct feedback to the individuals who you believe are misaligned. As much as they’re missing your context, you’re missing theirs, and a kind, clear conversation can often prevent years of unnecessary process.</li>\n<li><strong>Articulate your visions and strategies</strong> in a clear, helpful document. (This is a rich topic that I’ll be covering separately. <span style=\"text-decoration:underline;\">TODO: Add a link to writing technical strategies section once it’s written</span>.)</li>\n<li><strong>Encapsulate your approach in your workflows and tooling.</strong> Documentation of a clear vision is helpful, but some folks simply won’t study your document. Deliberate tools create workflows that nurture habits far better than training and documentation. For example, provisioning a new service might require going to a website that requires you to add a link to a technical spec for that service. Another approach might be blocking deploys to production if the service doesn’t have an on-call setup established, with someone currently on-call, and that individual must also have their push notifications enabled.</li>\n<li>**Train new team members during their onboarding. **Changing folks’ habits after they’ve formed is quite challenging, which is frustrating if you’re attempting to get folks to adopt new practices. However, if you get folks pointed in the right direction when they join, then that habit-momentum will work in favor of remaining aligned.</li>\n<li><strong>Use <a href=\"https://en.wikipedia.org/wiki/Conway%27s_law\">Conway’s Law</a>.</strong> Conway’s Law argues that organizations build software that reflects their structure. If your organization is poorly structured, this will lead to tightly coupled or tangled software. However, it’s also a force for quality if your organization’s design is an effective one.</li>\n<li><strong>Curate technology change</strong> using <a href=\"https://lethain.com/scaling-consistency/\">architecture reviews</a>, <a href=\"https://lethain.com/magnitudes-of-exploration/\">investment strategies</a>, and <a href=\"https://slack.engineering/how-big-technical-changes-happen-at-slack/\">a structured process for adopting new tools</a>. Most misalignment comes from missing context, and these are the organizational leverage points to inject context into decision-making. Many organizations start here, but it’s the last box of tools that I recommend opening. How can you provide consistent architecture reviews without an articulated vision? Why tell folks your strategy after they’ve designed something rather than in their onboarding process?</li>\n</ul>\n<p>Regardless of the approaches you use to align your technical vectors, this is work that tends to happen over months and years. There’s no world where you write the vision document and the org immediately aligns behind its brilliance. Much more likely is that it gathers dust until you invest in building support.</p>\n<p>Most companies can combine the above techniques from hot-spot fixing to vector-alignment into a successful approach for managing technical quality, and hopefully that’s the case for you. However, many find that they’re not enough and that you move towards heavier approaches. In that case, the first step is, as always, measurement.</p>\n<h2>Measure technical quality</h2>\n<p>The desire to measure in software engineering has generally outpaced our state of measurement. <a href=\"https://www.amazon.com/dp/B07B9F83WM/\">Accelerate</a> identifies metrics to measure velocity which are powerful for locating process and tooling problems, but these metrics start <em>after</em> the code’s been merged. How do you measure your codebase’s quality such that you can identify gaps, propose a plan of action, and evaluate the impact of your efforts to improve?</p>\n<p>There are some process measurements that correlate with effective changes. For example, you could measure the number of files changed in each pull request on the understanding that smaller pull requests are generally higher quality. You could also measure a codebase’s lines of code per file, on the assumption that very large files are generally hard to extend. These could both be quite helpful, and I’d even recommend measuring them, but I think they are at best proxy measurements for code quality.</p>\n<p>My experience is that it <em>is</em> possible to usefully measure codebase quality, and it comes down to developing an extremely precise definition of quality. The more detailed you can get your definition of quality, the more useful it becomes to measure a codebase, and the more instructive it becomes to folks hoping to improve the quality of the area they’re working on. This approach is described in some detail in <a href=\"https://www.amazon.com/Building-Evolutionary-Architectures-Support-Constant/dp/1491986360/\">Building Evolutionary Architectures</a> and <a href=\"https://lethain.com/reclaim-unreasonable-software/\">Reclaim unreasonable software</a>.</p>\n<p>Some representative components to consider including in your quality definition:</p>\n<ul>\n<li>What percentage of the code is statically typed?</li>\n<li>How many files have associated tests?</li>\n<li>What is test coverage within your codebase?</li>\n<li>How narrow are the public interfaces across modules?</li>\n<li>What percentage of files use the preferred HTTP library?</li>\n<li>Do endpoints respond to requests within 500ms after a cold start?</li>\n<li>How many functions have dangerous read-after-write behavior? Or perform unnecessary reads against the primary database instance?</li>\n<li>How many endpoints perform all state mutation within a single transaction?</li>\n<li>How many functions acquire low-granularity locks?</li>\n<li>How many hot files exist which are changed in more than half of pull requests?</li>\n</ul>\n<p>You’re welcome to disagree that some of these properties ought to exist in <em>your</em> codebase’s definition of quality: your definition should be specific to your codebase and your needs. The important thing is developing a precise, measurable definition. There will be disagreement in the development of that definition, and you will necessarily change the definition over time.</p>\n<p>After you’ve developed the definition, this is an area where instrumentation can be genuinely challenging, and instrumentation is a requirement for useful metrics. Instrumentation complexity is the biggest friction for adopting these techniques in practice, but if you can push through, you unlock something pretty phenomenal: a real, dynamic quality score that you can track over time and use to create a clarity of alignment in your approach that conceptual alignment cannot.</p>\n<p>With quality defined and instrumented, your next step is deciding between investing in a <em>quality team</em> or a <em>quality program</em>. A dedicated team is easy to coordinate and predictable in its bandwidth and is generally the easier place to start.</p>\n<h2>Technical quality team</h2>\n<p>A _technical quality team _is a software engineering team dedicated to creating quality in your codebase. You might call this team Developer Productivity, Developer Tools, or Product Infrastructure. In any case, the team’s goal is to create and preserve quality across your company’s software.</p>\n<p>This is not what’s sometimes called a quality assurance team. Although both teams make investments into tests, the technical quality team has a broader remit from workflow to build to test to interface design.</p>\n<p>When you’re bootstrapping such a team, start with a fixed team size of three to six folks. Having a small team forces you to relentlessly prioritize their roadmap on impact, and ensures you’ll maintain focus on the achievable. Over time this team will accumulate systems to maintain that require scaling investment, Jenkins clusters are a common example of this, and you’ll want to <a href=\"https://lethain.com/sizing-engineering-teams/\">size the team</a> as a function of the broader engineering organization. Rules of thumb are tricky here, but maybe one engineer working on developer tooling for every fifteen product engineers, and this is in addition to your infrastructure engineering investment.</p>\n<p>It’s rare for these teams to have a product manager, generally one-or-more Staff-plus engineers and the engineering manager partner to fill that role. Sometimes they employ a Technical Program Manager, but typically that is after they cross into operating a <em>Quality program</em> as described in the next section.</p>\n<p>When spinning up and operating one of these teams, some fundamentals of success are:</p>\n<ol>\n<li><strong>Trust metrics over intuition.</strong> You should have a way to measure every project. Quality is a complex system, the sort of place where your intuition can easily deceive you. Similarly, as you become more senior at your company, your experience will no longer reflect most other folks’ experiences. You already know about the rough edges and get priority help if you find a new one, but most other folks don’t. Metrics keep you honest.</li>\n<li>**Keep your intuition fresh. **Code and process change over time and your intuition is going stale every week you’re away from building product features. Most folks find that team embedding and team rotations are the best way to keep your instincts relevant. Others monitor chat for problems, as well as a healthy schedule of 1:1 discussions with product developers. The best folks do both of those and keep their metrics dashboards handy.</li>\n<li><strong>Listen to, and learn from, your users.</strong> There is a popular idea of “taste level,” which implies that some folks simply know what good looks like. There is a huge variance in folks who design effective quality investments, but rather than an innate skill, it usually comes from deeply understanding what your users are trying to accomplish <em>and</em> prioritizing that over your implementation constraints. <br>\n<br>\nAdoption and usability of your tools is much more important than raw power. A powerful tool that’s difficult to use will get a few power users, but most folks will pass it by. Slow down to get these details right. Hide all the <a href=\"https://en.wikipedia.org/wiki/No_Silver_Bullet\">accidental complexity</a>. Watch an engineer try to use your tool for their first time without helping them with it. Improve the gaps. Do that ten more times! If you’re not doing user research on your tools, then you are <em>doomed</em> as a quality investment team.</li>\n<li>**Do fewer things, but do them better. **When you’re building for the entire engineering organization, anything you do well will accelerate the overall organization. Anything you do poorly, including something almost great with too many rough edges, will drag everyone down. Although it’s almost always true that doing the few most important things will contribute more than many mediocre projects, this is even more true in cases where you’re trying to roll out tools and workflows to your entire organization (the organizational process-in-progress limits still apply here!).</li>\n<li><strong>Don’t horde impact.</strong> There’s a fundamental tension between centralized quality teams and the teams that they support. It’s often the case that there’s a globally optimal approach preferred by the centralized team which grates heavily on a subset of teams that work on atypical domains or workloads. One representative example is a company writing its backend servers in JavaScript and not allowing their machine learning engineers to use the Python ecosystem because they don’t want to support two ecosystems. Another case is a company standardized on using REST/HTTP2/JSON for all APIs where a particular team wants to use gRPC instead.  <br>\n<br>\nThere’s no perfect answer here, but it’s important to establish a thoughtful approach that <a href=\"http://lethain.com/magnitudes-of-exploration/\">balances the benefits of exploration against the benefits of standardization</a>.</li>\n</ol>\n<p>A successfully technical quality team using the above approaches will be <em>unquestionably _more productive than if the same number of engineers was directly doing product engineering work. Indeed, discounted developer productivity (in the spirit of [discounted cash flow](<a href=\"https://en.wikipedia.org/wiki/Discounted\">https://en.wikipedia.org/wiki/Discounted</a></em>cash_flow)) is the theoretically correct way to measure such a team’s impact. Only theoretically, because such calculations are mostly an evaluation of your self-confidence.</p>\n<p>Even if you’re quite successful, you’ll always have a backlog of high-impact work that you want to take on but don’t have the bandwidth to complete. Organizations don’t make purely rational team resourcing decisions, and you may find that you lack the bandwidth to complete important projects and likewise can’t get approval to hire additional folks onto your team.</p>\n<p>It’s generally a good sign that your team has more available high-impact work than you can take on, because it means you’re selective on what you do. This means you shouldn’t necessarily try to grow your technical quality team if you have a backlog. However, if you find that there is critical quality work that you can’t get to, then it may be time to explore starting a <em>quality program</em>.</p>\n<h2>Quality program</h2>\n<p>A <em>quality program</em> isn’t computer code at all, but rather an initiative led by a dedicated team to maintain technical quality across an organization. A quality program takes on the broad remit of achieving the organization’s target level of software quality. These are relatively uncommon, but something similar that you’ve probably encountered is an incident program responsible for a company’s incident retrospectives and remediations.</p>\n<p>Given this is written for an audience of senior technical leaders, let’s assume you have the technical perspective covered. Your next step is to find a Technical Program Manager who can co-lead the program and operate its mechanics. You can make considerable progress on the informational aspects of an organizational program without a Technical Program Manager; it’s a trap. You’ll be crushed by the coordination overhead of solo-driving a program in a large organization.</p>\n<p>Operating organizational programs is <a href=\"https://lethain.com/programs-owning-the-unownable/\">a broad topic about which much has been written</a>, but the core approach is: \\</p>\n<ol>\n<li><strong>Identify a program sponsor.</strong> You can’t change an organization’s behavior without an empowered sponsor. Organizations behave the way they do because it’s the optimal solution to their current constraints, and you can’t shift those constraints without the advocacy of someone powerful.</li>\n<li><strong>Generate sustainable, reproducible metrics.</strong> It’s common for folks running a program to spend four-plus hours a week maintaining their dataset by hand. This doesn’t work. Your data will have holes in it, you won’t be able to integrate your data with automation in later steps, and you’ll run out of energy to do the work to effect real change; refreshing a metrics dashboard has no inherent value.</li>\n<li><strong>Identify program goals for every impacted team and a clear path for them to accomplish those goals.</strong> Your program has to identify specific goals for each impacted team**, **for example, reducing test flakiness in their tests or closing incident remediations more quickly. However, it’s essential that you provide the map to success! So many programs make a demand of teams without providing direction on how to accomplish those goals. The program owner is the subject matter expert, don’t offload your strategy to every team to independently reinvent.</li>\n<li>**Build the tools and documentation to support teams towards their goals. **Once you’ve identified a clear path for teams to accomplish towards your program goals, figure out how you can help them make those changes! This might be providing “golden examples” of what things ought to look like, or even better, an example pull request refactoring a challenging section of code into the new pattern. It might be providing a test script to verify the migration worked correctly. It might be auto-generated the conversion commit to test, verify, and merge without having to write it themselves. Do as much as you possibly can to avoid every team having to deeply understand the problem space you’re attempting to make progress in.</li>\n<li><strong>Create a goal dashboard and share it widely.</strong> Once you have your program goals communicated to each team, you have to provide dashboards that help them understand their current state, their goal, and give reinforcing feedback on their (hopeful) progress along the way. The best dashboard is going to be both a scorecard for each team’s work and also provide breadcrumbs for each team on where to focus their next efforts. <br>\n<br>\nThere are three distinct zoom-levels that your dashboard should support. The fully zoomed-out level helps you evaluate your program’s impact. The fully zoomed-in level helps an individual team understand their remaining work. A third level between the two helps organizational leaders hold their teams accountable (and to support your program sponsor in making concrete, specific asks to hold those leaders accountable).</li>\n<li>**Send programmatic nudges for folks behind on their goals. **Folks are busy. They won’t always prioritize your program’s goals. Alternatively, they might do an amazing job of making your requested improvements but backtrack later with deprecated practices. Use nudges to direct the attention of teams towards the next work they should take towards your program’s goals. Remember, attention is a scarce resource! If you waste folks times with a nudge email or ping, they won’t pay attention to the next one.</li>\n<li><strong>Periodically review program status with your sponsor.</strong> Programs are trying to make progress on an organizational priority that doesn’t naturally align with the teams’ goals. Many teams struggle to break from their local prioritization to accomplish global priorities. This is where it’s essential to review your overall progress with your sponsor and point them towards the teams that prioritize program work. Effectively leveraging your sponsor to bridge misaligned prioritization will be essential to your success.</li>\n</ol>\n<p>In a lot of ways, a program is just an endless migration, and <a href=\"http://lethain.com/migrations/\">the techniques that apply to migrations work for programs as well</a>.</p>\n<p>If you get all of those steps right, you’re running a genuinely great program. This might feel like a lot of work, and wow, it is: a lot of programs go wrong. The three leading causes of failed programs are:</p>\n<ol>\n<li>running it purely from a process perspective and becoming detached from how the reality of what you’re trying to accomplish,</li>\n<li>running it purely from a technical perspective and thinking that you can skip the essential steps of advocating for your goal and listening to the folks you’re trying to motivate,</li>\n<li>trying to cover both perspectives as a single person--don’t go it alone!</li>\n</ol>\n<p>A bad program is a lot like an inefficient non-profit: the goal is right, but few funds reach the intended goal. No matter how you decide to measure technical quality, the most important thing to always remember when running your quality program is that the program isn’t the goal. The goal is creating technical quality. Organizational programs are massive and build so much momentum that inertia propels them forward long after they’ve stopped working. Keep your program lean enough to cancel, and remain self-critical enough to cancel it ceases driving quality creation.</p>\n<h2>Start small and add slowly</h2>\n<p>When you realize your actual technical quality has fallen considerably behind your target technical quality, the natural first reaction is to panic and start rolling out a vast array of techniques and solutions. Dumping all your ingredients into the pot, inevitably, doesn’t work well, and worse, you don’t even know which parts to keep.</p>\n<p>If you find yourself struggling with technical quality--and we all do, frequently--then start with something small, and iterate on it until it works. Then add another technique, iterate on that too. Slowly build towards something that genuinely works, even if it means weathering accusations of not moving fast enough. When it comes to complex systems and interdependencies, moving quickly is just optics, it’s moving slowly that gets the job done.</p>","frontmatter":{"date":"October 17, 2020","slug":"/guides/manage-technical-quality","name":null,"role":null,"kind":"guide","title":"Manage technical quality"}}},"pageContext":{"slug":"/guides/manage-technical-quality"}}}